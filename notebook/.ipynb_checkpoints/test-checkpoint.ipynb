{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "import json\n",
    "\n",
    "from src.utils.collate_fn_coco import collate_fn_coco\n",
    "from src.models.boxclip import BOXCLIP\n",
    "from src.models.transformer import *\n",
    "from src.utils.get_model_and_data import get_model_and_data\n",
    "\n",
    "from d2l import torch as d2l\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=16.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.01s)\n",
      "creating index...\n",
      "index created!\n",
      "train set scale: 21391\n",
      "loading annotations into memory...\n",
      "Done (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "val set scale: 925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "parameters = {'device': device}\n",
    "model, datasets = get_model_and_data(parameters)\n",
    "\n",
    "checkpoint_path = \"../checkpoint/07-27-00-39/checkpoint-epoch40.pth.tar\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1469259159.py, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    #         break\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(datasets['train'], batch_size=5, shuffle=True, \n",
    "                                         collate_fn=collate_fn_coco)\n",
    "\n",
    "num_img = 5\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# model = BOXCLIP(encoder, decoder, categories=categories, device=device).to(device)\n",
    "# model.load_state_dict(torch.load(\"./model/bbox-cats-10eps.pkl\"))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = model(batch)\n",
    "        for num_img in range(5):\n",
    "\n",
    "            plt.subplot(2, 5, num_img+1)\n",
    "            plt.imshow(to_pil_image(batch['clip_images'][num_img]))\n",
    "\n",
    "            for bbox, cat_id in batch['bboxes'][num_img]:\n",
    "\n",
    "                [bbox_x, bbox_y, bbox_w, bbox_h] = bbox\n",
    "                w, h = 224, 224\n",
    "    #             print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "                plt.gca().add_patch(plt.Rectangle((bbox_x*w, bbox_y*h), bbox_w*w, bbox_h*h, \n",
    "                                                  fill=False, edgecolor='r', linewidth=2))\n",
    "#             plt.title('\\n'.join([categories[cat_id] for _, cat_id in batch['bboxes'][num_img]]))\n",
    "\n",
    "        for num_img in range(5):\n",
    "            plt.subplot(2, 5, num_img+6)\n",
    "            plt.imshow(to_pil_image(batch['clip_images'][num_img]))\n",
    "\n",
    "    #         print(batch['output'].shape)\n",
    "            for j in range(batch['output_bbox'][num_img].shape[0]):\n",
    "                [bbox_x, bbox_y, bbox_w, bbox_h] = batch['output_bbox'][num_img][j].cpu()\n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "                plt.gca().add_patch(plt.Rectangle((bbox_x*w, bbox_y*h), bbox_w*w, bbox_h*h, \n",
    "                                                  fill=False, edgecolor='r', linewidth=2))\n",
    "            pred_cat = []\n",
    "            for i in range(batch['output_cat'][num_img].shape[0]):\n",
    "#                 print(batch['output_cat'][num_img][i].item())\n",
    "#                 pred_cat.append(categories[batch['output_cat'][num_img][i].item()])\n",
    "#             plt.title('\\n'.join(pred_cat))\n",
    "#         print(batch['output_cat'])\n",
    "\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip",
   "language": "python",
   "name": "motionclip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
