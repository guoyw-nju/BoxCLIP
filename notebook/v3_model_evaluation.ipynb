{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from src.utils.get_model_and_data import get_model_and_data\n",
    "from src.utils.collate_fn_coco import collate_fn_coco\n",
    "from src.parser.training import parser\n",
    "\n",
    "import clip\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../checkpoint/align_v3_gen_0_align_0_sample_100/checkpoint-epoch2000.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 10, 'bbox_mse': 5.0, 'bbox_mse_gen': 0.0, 'cats_cos': 1.0, 'cats_cos_gen': 0.0, 'checkpoint_path': None, 'checkpoint_step': 1000, 'clip_image_cosine': 0.0, 'clip_text_cosine': 0.0, 'device': 'cuda', 'exp_name': 'align_v3_gen_0_align_0_sample_100', 'exp_path': './checkpoint/align_v3_gen_0_align_0_sample_100', 'folder': './checkpoint', 'lambdas': {'bbox_mse': 5.0, 'bbox_mse_gen': 0.0, 'cats_cos': 1.0, 'cats_cos_gen': 0.0, 'clip_image_cosine': 0.0, 'clip_text_cosine': 0.0}, 'lr': 0.0001, 'lr_scheduler': None, 'no_val_loss': True, 'num_attentionLayer': 4, 'num_epochs': 2000, 'overfit': False, 'overfit_size': 100}\n",
      "loading annotations into memory...\n",
      "Done (t=17.85s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.13s)\n",
      "creating index...\n",
      "index created!\n",
      "train set scale: 21391\n",
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "val set scale: 925\n",
      "Total params: 178.09M\n",
      "Trainable params: 26.82M\n"
     ]
    }
   ],
   "source": [
    "folder, _ = os.path.split(checkpoint_path)\n",
    "opt_path = os.path.join(folder, 'opt.yaml')\n",
    "with open(opt_path, 'r', encoding='utf-8') as f:\n",
    "    parameters = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "parameters['overfit'] = False\n",
    "print(parameters)\n",
    "\n",
    "model, datasets = get_model_and_data(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets['train'][0]\n",
    "cat_texts = [v for k, v in datasets['train'].cats.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = iter(torch.utils.data.DataLoader(datasets['train'], batch_size=20, shuffle=False, collate_fn=collate_fn_coco))\n",
    "batch = next(dataloader)\n",
    "# batch = next(dataloader)\n",
    "# batch = next(iter(dataloader))\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v): batch[k] = batch[k].to('cuda')\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = model(batch)\n",
    "out_cats = model.feat2cat(batch, cat_texts)\n",
    "\n",
    "# [[cat_texts[j.item()] for j in i] for i in out_cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "# show some data point\n",
    "# batch = next(iter(dataloader))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.imshow(batch['clip_images'][i].permute(1,2,0).cpu())\n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['bboxs'][i][j]\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction\n",
    "# batch = next(iter(dataloader))\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v): batch[k] = batch[k].to('cuda')\n",
    "        \n",
    "num_img = 5\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = model(batch)\n",
    "    out_cats = model.feat2cat(batch, cat_texts)\n",
    "    out_cats = [[cat_texts[j.item()] for j in i] for i in out_cats]\n",
    "\n",
    "# original bboxs\n",
    "for num_img in range(5):\n",
    "    plt.subplot(2, 5, num_img+1)\n",
    "    plt.imshow(batch['clip_images'][num_img].cpu().permute(1,2,0))\n",
    "\n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['bboxs'][num_img][j]\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=2))\n",
    "        \n",
    "# output bboxs\n",
    "for num_img in range(5):\n",
    "    plt.subplot(2, 5, num_img+6)\n",
    "    plt.imshow(batch['clip_images'][num_img].cpu().permute(1,2,0))\n",
    "\n",
    "    for j in range(batch['output_bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['output_bboxs'][num_img][j].cpu()\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=2))\n",
    "    plt.title('\\n'.join(out_cats[num_img]), color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# text2bboxs\n",
    "# batch = next(iter(dataloader))\n",
    "#  'a woman standing on skiis while posing for the camera',\n",
    "#  'A white oven and a white refrigerator are in the kitchen.',\n",
    "#  'part of a sandwich sitting on a table',\n",
    "#  'A paddle boarder on a large, still body of water.',\n",
    "#  'A black fluffy cat sitting on top of a computer keyboard.',\n",
    "#  \"A baby giraffe drinking milk from it's mother in a field.\",\n",
    "#  'A white bowl of green granny smith apples.',\n",
    "#  'The boy is looking back at a wave in the ocean. ']\n",
    "text = [\n",
    "#         'cat and dog.',\n",
    "#         'a man with a dog.',\n",
    "#         'a horse standing by a tree.',\n",
    "#         'a woman having dinner.',\n",
    "#         'a photo of the bathroom.', \n",
    "#         'a picture of a kitchen.',\n",
    "        'two same and identical farm animals.',\n",
    "        'two different farm animals.',\n",
    "        \"a picture of two animals.\",\n",
    "        \"a picture of two animals that is human's best friend.\",\n",
    "#         \"a picture of two furry animals that eat mouses.\",\n",
    "#         'a picture of some farm animals except cow.',\n",
    "    \n",
    "        'a picture of some wild animals.',\n",
    "        'a picture of some different wild animals.',\n",
    "        'a picture of some transports.',\n",
    "        'a picture of some transports that can fly.',\n",
    "\n",
    "        'part of a sandwich sitting on a table',\n",
    "        'part of a sandwich sitting under a table',\n",
    "        'a woman standing on a chair.',\n",
    "        'a woman standing on the left of a chair.',\n",
    "    \n",
    "#         'A paddle boarder on a large, still body of water.',\n",
    "        'A cat sitting on top of a computer keyboard.',\n",
    "        'A cat sitting under a computer keyboard.',\n",
    "#         'a cat that is on the left in the photo.',\n",
    "        'a picture where two cats are on the right.',\n",
    "        'a picture where two cats are on the left.'\n",
    "#         'A cat sitting on the right of a computer keyboard.',\n",
    "#         'A cat sitting on the left of a computer keyboard.',\n",
    "#         'A black fluffy cat sitting on top of a computer keyboard.',\n",
    "#         'A black fluffy cat sitting under a computer keyboard.',\n",
    "#         \"A baby giraffe drinking milk from it's mother in a field.\",\n",
    "#         'A white bowl of green granny smith apples.',\n",
    "#         'The boy is looking back at a wave in the ocean. '\n",
    "]\n",
    "# batch = next(dataloader)\n",
    "text = []\n",
    "# text += [t[0] for t in batch['clip_texts'][:16]]\n",
    "\n",
    "text_feats = [clip.tokenize(t).cuda() for t in batch['clip_texts'][:16]]\n",
    "print(text_feats[0].shape, len(text_feats)) # (5, 77)\n",
    "\n",
    "text_feats = [model.clip_model.encode_text(t).mean(dim=0) for t in text_feats]\n",
    "text_feats = torch.stack(text_feats).float()\n",
    "print(text_feats.shape)\n",
    "# text_feats = clip.tokenize(text).to('cuda')\n",
    "# text_feats = model.clip_model.encode_text(text_feats).float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch.update(model.generate(text_feats))\n",
    "    \n",
    "    out_cats = model.feat2cat(batch, cat_texts)\n",
    "    out_cats = [[cat_texts[j.item()] for j in i] for i in out_cats]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "color = ('r', 'c')\n",
    "for num_img in range(16):\n",
    "    plt.subplot(4,4,num_img+1)\n",
    "#     plt.title(text[num_img], size=11)\n",
    "    plt.xlim(0, 224)\n",
    "    plt.ylim(0, 224)\n",
    "    \n",
    "    plt.gca().xaxis.set_ticks_position('top') #将x轴的位置设置在顶部\n",
    "    plt.gca().invert_yaxis()\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.gca().invert_yaxis()\n",
    "        \n",
    "    for j in range(batch['output_bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = torch.clamp(batch['output_bboxs'][num_img][j].cpu(), 0, 1)\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, color='r', linewidth=1))\n",
    "        plt.text((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224-3, out_cats[num_img][j], color='r', size=12)\n",
    "        \n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = torch.clamp(batch['bboxs'][num_img][j].cpu(), 0, 1)\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, color='c', linewidth=1))\n",
    "        plt.text((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224-3, batch['cat_name'][num_img][j], color='c', size=12)\n",
    "    \n",
    "#     plt.imshow(batch['clip_images'][num_img].permute(1,2,0).cpu())\n",
    "#     plt.title('\\n'.join(out_cats[num_img]), color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image2bboxs\n",
    "# batch = next(iter(dataloader))\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v): batch[k] = batch[k].to('cuda')\n",
    "\n",
    "model.eval()\n",
    "image_feats = model.clip_model.encode_image(batch['clip_images']).float()\n",
    "with torch.no_grad():\n",
    "    batch.update(model.generate(image_feats))\n",
    "    \n",
    "    out_cats = model.feat2cat(batch, cat_texts)\n",
    "    out_cats = [[cat_texts[j.item()] for j in i] for i in out_cats]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(batch['clip_images'][i].permute(1,2,0).cpu())\n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['bboxs'][i][j].cpu()\n",
    "#         print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "        plt.gca().add_patch(plt.Rectangle((bbox_x*224, bbox_y*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='c', linewidth=1))\n",
    "        plt.text(bbox_x*224, bbox_y*224-3, batch['cat_name'][i][j], color='c', size=12)\n",
    "    \n",
    "    for j in range(batch['output_bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = torch.clamp(batch['output_bboxs'][i][j].cpu(), 0, 1)\n",
    "        plt.gca().add_patch(plt.Rectangle((bbox_x*224, bbox_y*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=1))\n",
    "        plt.text(bbox_x*224, bbox_y*224-3, out_cats[i][j], color='r', size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
