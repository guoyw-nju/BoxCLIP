{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.utils.eval_accu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e178b99d706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn_coco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate_fn_coco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_accu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_accu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.utils.eval_accu'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from src.utils.get_model_and_data import get_model_and_data\n",
    "from src.utils.collate_fn_coco import collate_fn_coco\n",
    "from src.parser.training import parser\n",
    "from src.utils.eval_accu import eval_accu\n",
    "import clip\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../checkpoint/align_v4_clipfc_gen_recon/checkpoint-epoch100.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder, _ = os.path.split(checkpoint_path)\n",
    "opt_path = os.path.join(folder, 'opt.yaml')\n",
    "with open(opt_path, 'r', encoding='utf-8') as f:\n",
    "    parameters = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "parameters['overfit'] = False\n",
    "print(parameters)\n",
    "\n",
    "model, datasets = get_model_and_data(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets['train'][0]\n",
    "cat_texts = [v for k, v in datasets['train'].cats.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = iter(torch.utils.data.DataLoader(datasets['train'], batch_size=20, shuffle=False, collate_fn=collate_fn_coco))\n",
    "batch = next(dataloader)\n",
    "# batch = next(dataloader)\n",
    "# batch = next(iter(dataloader))\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v): batch[k] = batch[k].to('cuda')\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = model(batch)\n",
    "# out_cats = model.feat2cat(batch, cat_texts)\n",
    "\n",
    "# [[cat_texts[j.item()] for j in i] for i in out_cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(datasets['val'], batch_size=64, shuffle=False, collate_fn=collate_fn_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.eval_accu import eval_accu\n",
    "\n",
    "overfit_loader = torch.utils.data.DataLoader(datasets['train'], batch_size=10, shuffle=False, collate_fn=collate_fn_coco)\n",
    "# overfit_loader = torch.utils.data.DataLoader([datasets['train'][i] for i in range(10)], batch_size=10, shuffle=False, collate_fn=collate_fn_coco)\n",
    "eval_accu(overfit_loader, model, cat_texts, 'cuda', type='generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some data point\n",
    "# batch = next(iter(dataloader))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    _ = plt.imshow(batch['clip_images'][i].permute(1,2,0).cpu())\n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['bboxs'][i][j]\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction\n",
    "# batch = next(iter(dataloader))\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v): batch[k] = batch[k].to('cuda')\n",
    "        \n",
    "num_img = 5\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = model(batch)\n",
    "    batch.update(model.feat2cat(batch['output_cat_feats'], cat_texts))\n",
    "#     out_cats = [[cat_texts[j.item()] for j in i] for i in out_cats]\n",
    "\n",
    "# original bboxs\n",
    "for num_img in range(5):\n",
    "    plt.subplot(2, 5, num_img+1)\n",
    "    plt.imshow(batch['clip_images'][num_img].cpu().permute(1,2,0))\n",
    "\n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['bboxs'][num_img][j]\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=2))\n",
    "        \n",
    "# output bboxs\n",
    "for num_img in range(5):\n",
    "    plt.subplot(2, 5, num_img+6)\n",
    "    plt.imshow(batch['clip_images'][num_img].cpu().permute(1,2,0))\n",
    "\n",
    "    for j in range(batch['output_bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['output_bboxs'][num_img][j].cpu()\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=2))\n",
    "    plt.title('\\n'.join(batch['output_cat_name'][num_img]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# text2bboxs\n",
    "# batch = next(iter(dataloader))\n",
    "#  'a woman standing on skiis while posing for the camera',\n",
    "#  'A white oven and a white refrigerator are in the kitchen.',\n",
    "#  'part of a sandwich sitting on a table',\n",
    "#  'A paddle boarder on a large, still body of water.',\n",
    "#  'A black fluffy cat sitting on top of a computer keyboard.',\n",
    "#  \"A baby giraffe drinking milk from it's mother in a field.\",\n",
    "#  'A white bowl of green granny smith apples.',\n",
    "#  'The boy is looking back at a wave in the ocean. ']\n",
    "text = [\n",
    "#         'cat and dog.',\n",
    "#         'a man with a dog.',\n",
    "#         'a horse standing by a tree.',\n",
    "#         'a woman having dinner.',\n",
    "#         'a photo of the bathroom.', \n",
    "#         'a picture of a kitchen.',\n",
    "        'two same and identical farm animals.',\n",
    "        'two different farm animals.',\n",
    "        \"a picture of two animals.\",\n",
    "        \"a picture of two animals that is human's best friend.\",\n",
    "#         \"a picture of two furry animals that eat mouses.\",\n",
    "#         'a picture of some farm animals except cow.',\n",
    "    \n",
    "        'a picture of some wild animals.',\n",
    "        'a picture of some different wild animals.',\n",
    "        'a picture of some transports.',\n",
    "        'a picture of some transports that can fly.',\n",
    "\n",
    "        'part of a sandwich sitting on a table',\n",
    "        'part of a sandwich sitting under a table',\n",
    "        'a woman standing on a chair.',\n",
    "        'a woman standing on the left of a chair.',\n",
    "    \n",
    "#         'A paddle boarder on a large, still body of water.',\n",
    "        'A cat sitting on top of a computer keyboard.',\n",
    "        'A cat sitting under a computer keyboard.',\n",
    "#         'a cat that is on the left in the photo.',\n",
    "        'a picture where two cats are on the right.',\n",
    "        'a picture where two cats are on the left.'\n",
    "#         'A cat sitting on the right of a computer keyboard.',\n",
    "#         'A cat sitting on the left of a computer keyboard.',\n",
    "#         'A black fluffy cat sitting on top of a computer keyboard.',\n",
    "#         'A black fluffy cat sitting under a computer keyboard.',\n",
    "#         \"A baby giraffe drinking milk from it's mother in a field.\",\n",
    "#         'A white bowl of green granny smith apples.',\n",
    "#         'The boy is looking back at a wave in the ocean. '\n",
    "]\n",
    "# batch = next(dataloader)\n",
    "text = []\n",
    "# text += [t[0] for t in batch['clip_texts'][:16]]\n",
    "\n",
    "# mean\n",
    "# text_feats = [clip.tokenize(t).cuda() for t in batch['clip_texts'][:16]]\n",
    "# print(text_feats[0].shape, len(text_feats)) # (5, 77)\n",
    "\n",
    "# text_feats = [model.clip_model.encode_text(t).mean(dim=0) for t in text_feats] # [(5, 512)]\n",
    "# text_feats = torch.stack(text_feats).float()\n",
    "# print(text_feats.shape)\n",
    "\n",
    "# one caption\n",
    "text_feats = [clip.tokenize(t[0]).cuda() for t in batch['clip_texts'][:16]]\n",
    "print(text_feats[0].shape, len(text_feats)) # (5, 77)\n",
    "\n",
    "text_feats = [model.clip_model.encode_text(t).mean(dim=0) for t in text_feats] # [(5, 512)]\n",
    "text_feats = torch.stack(text_feats).float()\n",
    "print(text_feats.shape)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch.update(model.generate(text_feats, 'text'))\n",
    "    \n",
    "    batch.update(model.feat2cat(batch['output_cat_feats'], cat_texts))\n",
    "#     out_cats = [[cat_texts[j.item()] for j in i] for i in out_cats]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "color = ('r', 'c')\n",
    "for num_img in range(16):\n",
    "    plt.subplot(4,4,num_img+1)\n",
    "#     plt.title(text[num_img], size=11)\n",
    "    plt.xlim(0, 224)\n",
    "    plt.ylim(0, 224)\n",
    "    \n",
    "    plt.gca().xaxis.set_ticks_position('top') #将x轴的位置设置在顶部\n",
    "    plt.gca().invert_yaxis()\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.gca().invert_yaxis()\n",
    "        \n",
    "    for j in range(batch['output_bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = torch.clamp(batch['output_bboxs'][num_img][j].cpu(), 0, 1)\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, color='r', linewidth=1))\n",
    "        plt.text((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224-3, batch['output_cat_name'][num_img][j], color='r', size=12)\n",
    "        \n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = torch.clamp(batch['bboxs'][num_img][j].cpu(), 0, 1)\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, color='c', linewidth=1))\n",
    "        plt.text((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224-3, batch['cat_name'][num_img][j], color='c', size=12)\n",
    "    \n",
    "#     plt.imshow(batch['clip_images'][num_img].permute(1,2,0).cpu())\n",
    "#     plt.title('\\n'.join(out_cats[num_img]), color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch.update(model.generate(text_feats, 'text'))\n",
    "    \n",
    "    out_cats = model.feat2cat(batch, cat_texts)\n",
    "#     out_cats = [[cat_texts[j.item()] for j in i] for i in out_cats]\n",
    "\n",
    "for i in out_cats:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image2bboxs\n",
    "# batch = next(iter(dataloader))\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v): batch[k] = batch[k].to('cuda')\n",
    "\n",
    "model.eval()\n",
    "image_feats = model.clip_model.encode_image(batch['clip_images']).float()\n",
    "with torch.no_grad():\n",
    "    batch.update(model.generate(image_feats, 'image'))\n",
    "    \n",
    "    out_cats = model.feat2cat(batch, cat_texts)\n",
    "    out_cats = [[cat_texts[j.item()] for j in i] for i in out_cats]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(batch['clip_images'][i].permute(1,2,0).cpu())\n",
    "    for j in range(batch['bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = batch['bboxs'][i][j].cpu()\n",
    "#         print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='c', linewidth=1))\n",
    "        plt.text((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224-3, batch['cat_name'][i][j], color='c', size=12)\n",
    "    \n",
    "    for j in range(batch['output_bboxs'].shape[1]):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = torch.clamp(batch['output_bboxs'][i][j].cpu(), 0, 1)\n",
    "        plt.gca().add_patch(plt.Rectangle(((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224), bbox_w*224, bbox_h*224, \n",
    "                                          fill=False, edgecolor='r', linewidth=1))\n",
    "        plt.text((bbox_x-bbox_w/2)*224, (bbox_y-bbox_h/2)*224-3, out_cats[i][j], color='r', size=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
